# bruce_chat_app.py
import streamlit as st
from pathlib import Path
from huggingface_hub import InferenceClient

# ---------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ----------
st.set_page_config(page_title="Ø¨Ø±ÙˆØ³ ğŸ˜„", page_icon="ğŸ˜„", layout="centered")

# (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø´Ø¹Ø§Ø± Ù„Ùˆ Ø­Ø¨ÙŠØª ØªØ­Ø·Ù‡ Ø¯Ø§Ø®Ù„ assets/logo.png
logo_path = Path("assets/logo.png")
if logo_path.exists():
    st.image(str(logo_path), width=120)

st.title("Ø¨Ø±ÙˆØ³ ğŸ˜„")
st.caption("MVP â€” Ù…Ø­Ø§Ø¯Ø«Ø© Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø³ÙŠØ·Ø© ØªØ¹Ù…Ù„ Ø¹Ø¨Ø± Hugging Face Inference")

# ---------- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙˆÙƒÙ† ----------
if "HF_TOKEN" not in st.secrets or not st.secrets["HF_TOKEN"]:
    st.error(
        "Ù…Ø§ Ù„Ù‚ÙŠØª Ù…ØªØºÙŠØ± HF_TOKEN ÙÙŠ Secrets.\n"
        "Ø±ÙˆØ­ Ù„Ù€ Manage app â–¸ Settings â–¸ Secrets ÙˆØ­Ø·:\n\nHF_TOKEN = \"hf_...\""
    )
    st.stop()
MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"
HF_TOKEN = st.secrets["HF_TOKEN"]

# ---------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ù„Ù…ÙˆØ¯ÙŠÙ„ ----------
# ØªÙ‚Ø¯Ø± ØªØºÙŠÙ‘Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù„Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ ÙŠØ¯Ø¹Ù… text-generation Ø¹Ù„Ù‰ Hugging Face

client = InferenceClient(model=MODEL_ID, token=HF_TOKEN)

# ---------- Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ----------
if "history" not in st.session_state:
    st.session_state.history = []  # Ø¹Ù†Ø§ØµØ± Ø¨Ø´ÙƒÙ„: [("user", "..."), ("assistant", "...")]

SYSTEM_PROMPT = (
    "Ø£Ù†Øª Ø¨Ø±ÙˆØ³ØŒ Ø±Ø¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ„Ø·ÙŠÙ ÙˆÙ…Ø®ØªØµØ±.\n"
    "Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù† Ø®Ø·ÙˆØ§Øª ØªÙ‚Ù†ÙŠØ©ØŒ Ø§Ø¹Ø·Ù‡ Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù…Ø© ÙˆØ¨Ø³ÙŠØ·Ø©.\n"
    "ØªØ¬Ù†Ø¨ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹."
)

def build_prompt(history: list[tuple[str, str]], user_text: str) -> str:
    """
    Ù†Ø¨Ù†ÙŠ Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¨Ø³ÙŠØ· Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù„ÙŠ ØªØ´ØªØºÙ„ Ø¨Ù†Ù…Ø· text-generation (Ù…Ùˆ Ù…Ø­Ø§Ø¯Ø«Ø© Ø£ØµÙ„ÙŠØ©).
    """
    parts = [SYSTEM_PROMPT, ""]
    for role, content in history:
        if role == "user":
            parts.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {content}")
        else:
            parts.append(f"Ø¨Ø±ÙˆØ³: {content}")
    parts.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_text}")
    parts.append("Ø¨Ø±ÙˆØ³:")
    return "\n".join(parts)

def generate_response(history: list[tuple[str, str]], user_text: str) -> str:
    prompt = build_prompt(history, user_text)
    # Ù†Ø·Ù„Ø¨ Ù†Øµ Ø¬Ø¯ÙŠØ¯ ÙÙ‚Ø· (Ø¨Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ)
    # Ù…Ù„Ø§Ø­Ø¸Ø©: Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ØªØªØ¹Ø§Ù…Ù„ Ø£ÙØ¶Ù„ Ù…Ø¹ do_sample=True ÙˆØ¨Ø¹Ø¶Ù‡Ø§ FalseØ› Ø¬Ø±Ù‘Ø¨ Ù„Ùˆ Ø­Ø¨ÙŠØª
    out = client.text_generation(
        prompt,
        max_new_tokens=256,
        temperature=0.2,
        top_p=0.9,
        repetition_penalty=1.1,
        do_sample=True,
        return_full_text=False,
    )
    # Ø¨Ø¹Ø¶ Ø§Ù„Ø³Ø±ÙØ±Ø§Øª ØªØ±Ø¬Ø¹ ÙƒØ§Ø¦Ù†/Ø¯ÙŠÙƒØªØŒ ÙˆØ¨Ø¹Ø¶Ù‡Ø§ Ø³ØªØ±ÙŠÙ†Øºâ€”Ù†Ø¶Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù†Øµ
    return str(out).strip()

# ---------- Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ----------
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ...", placeholder="Ù…Ø«Ù„Ø§Ù‹: Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø±ÙˆØ³", label_visibility="collapsed")
    submitted = st.form_submit_button("Ø¥Ø±Ø³Ø§Ù„")

# Ø²Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„ØªØ¬Ø§Ø±Ø¨
if st.button("Ø§Ù‡Ù„Ø§ Ø¨Ø±ÙˆØ³", use_container_width=True):
    st.session_state.history.append(("user", "Ø£Ù‡Ù„Ù‹Ø§ Ø¨Ø±ÙˆØ³"))
    try:
        reply = generate_response(st.session_state.history, "Ø£Ù‡Ù„Ù‹Ø§ Ø¨Ø±ÙˆØ³")
    except Exception as e:
        st.error(f"ØµØ§Ø± Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©:\n\n{e}")
    else:
        st.session_state.history.append(("assistant", reply))
        st.rerun()

# Ø¹Ø±Ø¶ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
for role, content in st.session_state.history:
    if role == "user":
        st.chat_message("user").markdown(content)
    else:
        st.chat_message("assistant").markdown(content)

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ Ù…Ù† Ø§Ù„Ø­Ù‚Ù„
if submitted and user_input.strip():
    st.chat_message("user").markdown(user_input)
    st.session_state.history.append(("user", user_input))
    try:
        reply = generate_response(st.session_state.history, user_input)
    except Exception as e:
        st.error(
            "ØµØ§Ø± Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©.\n\n"
            f"{e}\n\n"
            "Ø¥Ø°Ø§ Ø´ÙØª Ø±Ø³Ø§Ù„Ø© ÙÙŠÙ‡Ø§ 'not supported for task text-generation' ØºÙŠÙ‘Ø± MODEL_ID Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØ¯Ø¹Ù… text-generation."
        )
    else:
        st.session_state.history.append(("assistant", reply))
        st.chat_message("assistant").markdown(reply)

# Ø£Ø¯ÙˆØ§Øª Ø¬Ø§Ù†Ø¨ÙŠØ©
with st.sidebar:
    st.header("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    st.write(f"Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ: `{MODEL_ID}`")
    if st.button("Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.history = []
        st.experimental_rerun()
