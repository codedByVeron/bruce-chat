# -*- coding: utf-8 -*-
import streamlit as st
from huggingface_hub import InferenceClient

# -----------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
# -----------------------------
st.set_page_config(page_title="Ø¨Ù€Ù€Ø±ÙˆØ³ ğŸ˜„", page_icon="ğŸ¤–", layout="centered")

# Ø§Ù‚Ø±Ø£ Ù…ÙØªØ§Ø­ Hugging Face Ù…Ù† Ø§Ù„Ø³ÙŠÙƒØ±ØªØ³
if "HF_API_KEY" not in st.secrets:
    st.error('Ù…ÙØªØ§Ø­ Hugging Face Ù…ÙÙ‚ÙˆØ¯. Ø§Ø¯Ø®Ù„ Ø¹Ù„Ù‰ Manage app â†’ Settings â†’ Secrets ÙˆØ­Ø·:\nHF_API_KEY = "hf_..."')
    st.stop()

HF_KEY = st.secrets["HF_API_KEY"]

# Ø§Ø®ØªÙØ± Ù…ÙˆØ¯ÙŠÙ„ Ù…ØªØ§Ø­ Ø¹Ù„Ù‰ Inference API (Ù„Ø§ ÙŠØ­ØªØ§Ø¬ Ù…ÙˆØ§ÙÙ‚Ø© Ø®Ø§ØµØ©)
# Ù„Ùˆ ÙˆØ§Ø¬Ù‡Øª Ø¨Ø·Ø¡ Ø¬Ø±Ù‘Ø¨ ØªØ¨Ø¯Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø«Ø§Ù†ÙŠ
MODEL_ID = "google/gemma-2-2b-it"  # Ø®ÙŠØ§Ø± Ø®ÙÙŠÙ
# MODEL_ID = "HuggingFaceH4/zephyr-7b-beta"  # Ø¨Ø¯ÙŠÙ„

client = InferenceClient(api_key=HF_KEY)

SYSTEM_PROMPT = (
    "Ø§Ù†ØªÙ Ø¨Ø±ÙˆØ³ØŒ Ø±Ø¯Ù‘ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ø¹Ø±Ø¨ÙŠ Ø¨Ø³ÙŠØ·ØŒ Ù„Ø·ÙŠÙØŒ ÙˆÙ…Ø¨Ø§Ø´Ø±. "
    "Ø§Ø®ØªØµØ± Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†ØŒ ÙˆØ§Ø°Ø§ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø®Ø·ÙˆØ§Øª ÙØ§Ø¹Ø·ÙÙ‡Ø§ Ù…Ø±Ù‚Ù‘Ù…Ø©. "
    "Ù„Ø§ ØªØ°ÙƒØ± Ø³ÙŠØ§Ø³Ø§Øª Ø£Ùˆ Ù…ÙØ§ØªÙŠØ­ Ø£Ùˆ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¯Ø§Ø®Ù„ÙŠØ©."
)

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
# -----------------------------
def chat_via_hf(messages, max_tokens=350, temperature=0.7):
    """
    Ù†Ø­Ø§ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹ chat_completion (Ù„Ùˆ Ù…ØªØ§Ø­Ø© ÙÙŠ Ù†Ø³Ø®ØªÙƒ Ù…Ù† huggingface_hub)ØŒ
    ÙˆÙ„Ùˆ ÙØ´Ù„Øª Ù†Ø±Ø¬Ø¹ Ù„Ø·Ø±ÙŠÙ‚Ø© text_generation Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ù…Ø­Ø§Ø¯Ø«Ø© Ø¨Ø³ÙŠØ·.
    """
    # 1) ØªØ¬Ø±Ø¨Ø© chat_completion (Ù‚Ø¯ Ù„Ø§ ØªØªÙˆÙØ± ÙÙŠ Ø§ØµØ¯Ø§Ø±Ø§Øª Ù‚Ø¯ÙŠÙ…Ø©)
    try:
        resp = client.chat_completion(
            model=MODEL_ID,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†Øµ
        return resp.choices[0].message.content.strip()
    except Exception:
        pass

    # 2)Fallback: text_generation Ù…Ø¹ Ù‚Ø§Ù„Ø¨ Ù…Ø­Ø§Ø¯Ø«Ø©
    # Ù†Ø­ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø¥Ù„Ù‰ Ø¨Ø±ÙˆÙ…Ø¨Øª ÙˆØ§Ø­Ø¯
    prompt_lines = [f"system: {SYSTEM_PROMPT}"]
    for m in messages:
        role = m.get("role", "user")
        content = m.get("content", "")
        prompt_lines.append(f"{role}: {content}")
    prompt_lines.append("assistant:")

    prompt = "\n".join(prompt_lines)

    out = client.text_generation(
        model=MODEL_ID,
        prompt=prompt,
        temperature=temperature,
        max_new_tokens=max_tokens,
        do_sample=True,
        return_full_text=False,
    )
    # Ø¨Ø¹Ø¶ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªØ±Ø¬Ø¹ string Ù…Ø¨Ø§Ø´Ø±Ø©
    if isinstance(out, str):
        return out.strip()
    # ÙˆØ¨Ø¹Ø¶Ù‡Ø§ dict/obj ÙÙŠÙ‡ "generated_text"
    gen = getattr(out, "generated_text", None) or out.get("generated_text", "")
    return (gen or "").strip()


def generate_response(history, user_text):
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    messages += history
    messages.append({"role": "user", "content": user_text})
    return chat_via_hf(messages)


# -----------------------------
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# -----------------------------
st.title("Ø¨Ù€Ù€Ø±ÙˆØ³ ğŸ˜„")
st.caption("MVP â€” Ù…Ø­Ø§Ø¯Ø«Ø© Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø³ÙŠØ·Ø© (ØªØ¹Ù…Ù„ Ø¹Ø¨Ø± Hugging Face Inference)")

# Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
if "history" not in st.session_state:
    st.session_state.history = []

# Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„
for turn in st.session_state.history:
    if turn["role"] == "user":
        st.chat_message("user").markdown(turn["content"])
    else:
        st.chat_message("assistant").markdown(turn["content"])

# Ø­Ù‚Ù„ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
user_msg = st.chat_input("Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ...")

if user_msg:
    # Ø£Ø¶Ù Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    st.session_state.history.append({"role": "user", "content": user_msg})
    st.chat_message("user").markdown(user_msg)

    with st.spinner("â³ ÙŠÙÙƒØ±..."):
        try:
            reply = generate_response(st.session_state.history, user_msg)
        except Exception as e:
            st.error(f"ØµØ§Ø± Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©: {e}")
            st.stop()

    # Ø£Ø¶Ù Ø±Ø¯ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    st.session_state.history.append({"role": "assistant", "content": reply})
    st.chat_message("assistant").markdown(reply)

# Ø´Ø±ÙŠØ· Ø¬Ø§Ù†Ø¨ÙŠ
with st.sidebar:
    st.header("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    if st.button("Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"):
        st.session_state.history = []
        st.experimental_rerun()

    st.caption(":bulb: ØªÙ„Ù…ÙŠØ­: Ù„Ùˆ Ø­Ø³Ù‘ÙŠØª Ø¨Ø·Ø¡ØŒ Ø¬Ø±Ù‘Ø¨ ØªØ¨Ø¯Ù‘Ù„ MODEL_ID ÙÙŠ Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ù…ÙˆØ¯ÙŠÙ„ Ø¢Ø®Ø± ÙŠØ¯Ø¹Ù… Inference API.")
